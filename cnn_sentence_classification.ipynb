{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-sentence-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbzGeVgKM-",
        "colab_type": "text"
      },
      "source": [
        "## **Source**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQR7jSS2Dee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/jojonki/cnn-for-sentence-classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScWRfGdqgSiW",
        "colab_type": "text"
      },
      "source": [
        "## **Setting up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhZuIQd1GTES",
        "colab_type": "code",
        "outputId": "dd6c5a35-5eb1-4b7f-afa1-ceffa01625fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "source": [
        "!mkdir datasets\n",
        "# !wget -O datasets/rt-polaritydata.tar.gz https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
        "# !tar -C datasets -xvf datasets/rt-polaritydata.tar.gz\n",
        "!unzip datasets.zip\n",
        "!wget -O datasets/glove.6B.zip http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip datasets/glove.6B.zip -d datasets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  datasets.zip\n",
            "   creating: datasets/rt-polaritydata/\n",
            "  inflating: datasets/rt-polaritydata/rt-polarity.pos  \n",
            "  inflating: datasets/rt-polaritydata/rt-polarity.neg  \n",
            "   creating: datasets/semeval/\n",
            "  inflating: datasets/semeval/Restaurants_Train_v2.xml.txt  \n",
            "  inflating: datasets/semeval/Laptops_test_PhaseB.xml.txt  \n",
            "  inflating: datasets/semeval/Laptops_test_PhaseA.xml.txt  \n",
            "  inflating: datasets/semeval/Restuarants_test_phaseA.xml.txt  \n",
            "  inflating: datasets/semeval/Restuarants_test_phaseB.xml.txt  \n",
            "  inflating: datasets/semeval/Laptops_train_v2.xml.txt  \n",
            "  inflating: datasets/semeval/Restaurants_Train.xml.txt  \n",
            "   creating: datasets/sentihood/\n",
            "  inflating: datasets/sentihood/sentihood-test.json  \n",
            "  inflating: datasets/sentihood/sentihood-dev.json  \n",
            "  inflating: datasets/sentihood/sentihood-train.json  \n",
            "   creating: datasets/sst/\n",
            "  inflating: datasets/sst/Test_SST-1.txt  \n",
            "  inflating: datasets/sst/Dev_SST-2.txt  \n",
            "  inflating: datasets/sst/Training_SST-1.txt  \n",
            "  inflating: datasets/sst/Training_SST-2.txt  \n",
            "  inflating: datasets/sst/Dev_SST-1.txt  \n",
            "  inflating: datasets/sst/Test_SST-2.txt  \n",
            "   creating: datasets/sst/.ipynb_checkpoints/\n",
            "--2020-01-16 17:05:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-01-16 17:05:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-01-16 17:05:46--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘datasets/glove.6B.zip’\n",
            "\n",
            "datasets/glove.6B.z 100%[===================>] 822.24M  1.99MB/s    in 6m 31s  \n",
            "\n",
            "2020-01-16 17:12:17 (2.10 MB/s) - ‘datasets/glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  datasets/glove.6B.zip\n",
            "  inflating: datasets/glove.6B.50d.txt  \n",
            "  inflating: datasets/glove.6B.100d.txt  \n",
            "  inflating: datasets/glove.6B.200d.txt  \n",
            "  inflating: datasets/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUQH0l7HgdbC",
        "colab_type": "text"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sNb1d1uIZ65",
        "colab_type": "code",
        "outputId": "a90d5e21-c38e-4e11-f2ae-ce366fc95044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TnPM4TKHvgC",
        "colab_type": "code",
        "outputId": "3b014a54-5321-4bb1-90c4-20277af566b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "import os\n",
        "import random\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Dense, Lambda, Permute, Dropout\n",
        "from keras.layers import Conv2D, MaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyfE5xtogk0h",
        "colab_type": "text"
      },
      "source": [
        "## **Loading Data from Different Datasets**\n",
        "To train for a specific dataset, only run the cell for that dataset, then skip over to the next section.\\\n",
        "\\Don't forget to run the first cells in this section for the loading functions.\\![alt text](https://)\n",
        "Data format is as follows: [ ( [word1, word2, word3], label ), ... ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je6Kyy9ZmEnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(data, sentence_maxlen, w2i):\n",
        "    vec_data = []\n",
        "    labels = []\n",
        "    for d, label in data:\n",
        "        vec = [w2i[w] for w in d if w in w2i]\n",
        "        pad_len = max(0, sentence_maxlen - len(vec))\n",
        "        vec += [0] * pad_len\n",
        "        vec_data.append(vec)\n",
        "        labels.append(label)\n",
        "    vec_data = np.array(vec_data)\n",
        "    labels = to_categorical(np.array(labels))\n",
        "    return vec_data, labels\n",
        "\n",
        "def setup_vocab(data):\n",
        "    sentence_maxlen = max(map(len, (d for d, _ in data)))\n",
        "    vocab = []\n",
        "    for d, _ in data:\n",
        "        for w in d:\n",
        "            if w not in vocab: vocab.append(w)\n",
        "    vocab = sorted(vocab)\n",
        "    vocab_size = len(vocab)\n",
        "    print('Sentence Max Length', sentence_maxlen)\n",
        "    print('Vocab Examples:', vocab[:10])\n",
        "    print('Vocab Size', len(vocab))\n",
        "    w2i = {w:i for i,w in enumerate(vocab)}\n",
        "    return sentence_maxlen, w2i, vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjWarsSJIjTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # repo data\n",
        "# def load_data(fpath, label):\n",
        "#     data = []\n",
        "#     with codecs.open(fpath, 'r', 'utf-8', errors='ignore') as f:\n",
        "#         lines = f.readlines()\n",
        "#         for l in lines:\n",
        "#             l = l.rstrip()\n",
        "#             data.append((l.split(' '), label))\n",
        "#     return data\n",
        "\n",
        "# pos = load_data('datasets/rt-polaritydata/rt-polarity.pos', 1)\n",
        "# neg = load_data('datasets/rt-polaritydata/rt-polarity.neg', 0)\n",
        "# data = pos + neg\n",
        "# classes = sorted(set([i[1] for i in data]))\n",
        "# num_classes = len(classes)\n",
        "# print(f'Loaded {len(data)} data rows with {num_classes} classes.')\n",
        "# print(f'Classes: {classes}')\n",
        "# print('Example:')\n",
        "# print(data[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNicup4BSty6",
        "colab_type": "code",
        "outputId": "afa85872-18c7-4ab6-c31d-ffa2b87ca572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# sst-1 data\n",
        "def load_data(paths):\n",
        "    data = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "            r = f.read()\n",
        "        for line in r.split('\\n'):\n",
        "            line = line.rstrip()\n",
        "            text = line.split(' ')[1:]\n",
        "            L = line.split(' ')[0]\n",
        "            label = int(L)\n",
        "            data.append((text, label))\n",
        "    classes = sorted(set([i[1] for i in data]))\n",
        "    num_classes = len(classes)\n",
        "    return data, classes, num_classes\n",
        "\n",
        "# load data\n",
        "data_train, classes, num_classes = load_data(['datasets/sst/Training_SST-1.txt'])\n",
        "data_valid, _, _ = load_data(['datasets/sst/Dev_SST-1.txt'])\n",
        "data_test, _, _ = load_data(['datasets/sst/Test_SST-1.txt'])\n",
        "# shuffle data\n",
        "random.shuffle(data_train)\n",
        "random.shuffle(data_valid)\n",
        "random.shuffle(data_test)\n",
        "# define vocab\n",
        "all_data = data_train + data_valid + data_test\n",
        "sentence_maxlen, w2i, vocab_size = setup_vocab(all_data)\n",
        "# vectorize data\n",
        "trainX, trainY = vectorize(data_train, sentence_maxlen, w2i)\n",
        "validX, validY = vectorize(data_valid, sentence_maxlen, w2i)\n",
        "testX, testY = vectorize(data_test, sentence_maxlen, w2i)\n",
        "# print summary\n",
        "print(f'Loaded {len(all_data)} data rows with {num_classes} classes.')\n",
        "print(f'Training Data: {len(data_train)}\\nValidation Data: {len(data_valid)}\\nTest Data: {len(data_test)}')\n",
        "print(f'Classes: {classes}')\n",
        "print('Example:')\n",
        "print(data_train[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Max Length 56\n",
            "Vocab Examples: ['!', '!?', '#', '$', '%', '&', \"'\", \"''\", \"'30s\", \"'40s\"]\n",
            "Vocab Size 19536\n",
            "Loaded 11858 data rows with 5 classes.\n",
            "Training Data: 8545\n",
            "Validation Data: 1102\n",
            "Test Data: 2211\n",
            "Classes: [0, 1, 2, 3, 4]\n",
            "Example:\n",
            "(['while', 'some', 'of', 'the', 'camera', 'work', 'is', 'interesting', ',', 'the', 'film', \"'s\", 'mid-to-low', 'budget', 'is', 'betrayed', 'by', 'the', 'surprisingly', 'shoddy', 'makeup', 'work', '.'], 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "840L-kegmNEv",
        "colab_type": "code",
        "outputId": "89b27feb-f030-40e9-b725-fbc9c2d99929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# sst-2 data\n",
        "def load_data(paths):\n",
        "    data = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "            r = f.read()\n",
        "        for line in r.split('\\n'):\n",
        "            line = line.rstrip()\n",
        "            text = line.split(' ')[1:]\n",
        "            L = line.split(' ')[0]\n",
        "            data.append((text, int(L)))\n",
        "    classes = sorted(set([i[1] for i in data]))\n",
        "    num_classes = len(classes)\n",
        "    return data, classes, num_classes\n",
        "\n",
        "# load data\n",
        "data_train, classes, num_classes = load_data(['datasets/sst/Training_SST-2.txt'])\n",
        "data_valid, _, _ = load_data(['datasets/sst/Dev_SST-2.txt'])\n",
        "data_test, _, _ = load_data(['datasets/sst/Test_SST-2.txt'])\n",
        "# shuffle data\n",
        "random.shuffle(data_train)\n",
        "random.shuffle(data_valid)\n",
        "random.shuffle(data_test)\n",
        "# define vocab\n",
        "all_data = data_train + data_valid + data_test\n",
        "sentence_maxlen, w2i, vocab_size = setup_vocab(all_data)\n",
        "# vectorize data\n",
        "trainX, trainY = vectorize(data_train, sentence_maxlen, w2i)\n",
        "validX, validY = vectorize(data_valid, sentence_maxlen, w2i)\n",
        "testX, testY = vectorize(data_test, sentence_maxlen, w2i)\n",
        "# print summary\n",
        "print(f'Loaded {len(all_data)} data rows with {num_classes} classes.')\n",
        "print(f'Training Data: {len(data_train)}\\nValidation Data: {len(data_valid)}\\nTest Data: {len(data_test)}')\n",
        "print(f'Classes: {classes}')\n",
        "print('Example:')\n",
        "print(data_train[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Max Length 56\n",
            "Vocab Examples: ['!', '!?', '#', '$', '%', '&', \"'\", \"''\", \"'30s\", \"'40s\"]\n",
            "Vocab Size 17573\n",
            "Loaded 9616 data rows with 2 classes.\n",
            "Training Data: 6921\n",
            "Validation Data: 873\n",
            "Test Data: 1822\n",
            "Classes: [0, 1]\n",
            "Example:\n",
            "(['i', 'liked', 'the', 'original', 'short', 'story', 'but', 'this', 'movie', ',', 'even', 'at', 'an', 'hour', 'and', 'twenty-some', 'minutes', ',', 'it', \"'s\", 'too', 'long', 'and', 'it', 'goes', 'nowhere', '.'], 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M8b3x2TnO1T",
        "colab_type": "code",
        "outputId": "baab648b-22b0-46b2-d86b-4edb2158afc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# sentihood data\n",
        "import json\n",
        "def load_data(paths):\n",
        "    data = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r') as f:\n",
        "            j = json.load(f)\n",
        "        for i in range(len(j)):\n",
        "            if len(j[i]['opinions']) > 0:\n",
        "                if j[i]['opinions'][0]['sentiment'] == 'Positive':\n",
        "                    label = 1\n",
        "                else:\n",
        "                    label = 0\n",
        "                text = j[i]['text'].strip()\n",
        "                for idx in range(len(j[i]['opinions'])):\n",
        "                    aspect = j[i]['opinions'][idx]['aspect']\n",
        "                    entity = j[i]['opinions'][idx]['target_entity']\n",
        "                    text = text.replace(entity, aspect)\n",
        "                data.append((text.rstrip().split(' '), label))\n",
        "    classes = sorted(set([i[1] for i in data]))\n",
        "    num_classes = len(classes)\n",
        "    return data, classes, num_classes\n",
        "\n",
        "# load data\n",
        "data_train, classes, num_classes = load_data(['datasets/sentihood/sentihood-train.json'])\n",
        "data_valid, _, _ = load_data(['datasets/sentihood/sentihood-dev.json'])\n",
        "data_test, _, _ = load_data(['datasets/sentihood/sentihood-test.json'])\n",
        "# shuffle data\n",
        "random.shuffle(data_train)\n",
        "random.shuffle(data_valid)\n",
        "random.shuffle(data_test)\n",
        "# define vocab\n",
        "all_data = data_train + data_valid + data_test\n",
        "sentence_maxlen, w2i, vocab_size = setup_vocab(all_data)\n",
        "# vectorize data\n",
        "trainX, trainY = vectorize(data_train, sentence_maxlen, w2i)\n",
        "validX, validY = vectorize(data_valid, sentence_maxlen, w2i)\n",
        "testX, testY = vectorize(data_test, sentence_maxlen, w2i)\n",
        "# print summary\n",
        "print(f'Loaded {len(all_data)} data rows with {num_classes} classes.')\n",
        "print(f'Training Data: {len(data_train)}\\nValidation Data: {len(data_valid)}\\nTest Data: {len(data_test)}')\n",
        "print(f'Classes: {classes}')\n",
        "print('Example:')\n",
        "print(data_train[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Max Length 115\n",
            "Vocab Examples: ['', '\"', '\"Awarded', '\"Tourist', '\"convenient\"', '\"dangerous\"', '\"murder', '\"nicer\"', '\"normal\"', '\"open\"']\n",
            "Vocab Size 4994\n",
            "Loaded 3529 data rows with 2 classes.\n",
            "Training Data: 2021\n",
            "Validation Data: 505\n",
            "Test Data: 1003\n",
            "Classes: [0, 1]\n",
            "Example:\n",
            "(['Personally', 'I', 'would', 'rather', 'live', 'in', 'transit-location,', 'but', 'just', 'cus', 'its', 'more', 'central', 'then', 'transit-location'], 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFLOLo6Gt-PC",
        "colab_type": "code",
        "outputId": "008b9f44-b252-46bb-ad00-8a0f73717d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# semeval data\n",
        "import xml.etree.ElementTree as ET\n",
        "def load_data(paths):\n",
        "    data = []\n",
        "    for path in paths:\n",
        "        tree = ET.parse(path)\n",
        "        root = tree.getroot()\n",
        "        for sentence in root.iter('sentence'):\n",
        "            text = sentence.find('text').text.strip()\n",
        "            P = []\n",
        "            aspects = sentence.find('aspectTerms')\n",
        "            if aspects is not None:\n",
        "                for aspect in aspects:\n",
        "                    P.append(aspect.get('polarity'))\n",
        "            if 'negative' in P and not 'positive' in P:\n",
        "                polarity = 0\n",
        "            elif 'positive' in P and not 'negative' in P:\n",
        "                polarity = 1\n",
        "            else:\n",
        "                polarity = 2\n",
        "            data.append((text.rstrip().split(' '), polarity))\n",
        "    classes = sorted(set([i[1] for i in data]))\n",
        "    num_classes = len(classes)\n",
        "    return data, classes, num_classes\n",
        "\n",
        "# load data\n",
        "data, classes, num_classes = load_data(['datasets/semeval/Laptops_train_v2.xml.txt', 'datasets/semeval/Restaurants_Train_v2.xml.txt'])\n",
        "# shuffle data\n",
        "random.shuffle(data)\n",
        "# define vocab\n",
        "sentence_maxlen, w2i, vocab_size = setup_vocab(data)\n",
        "# split dataset into train-valid-test (80-10-10)\n",
        "data_train, data_valid, data_test = np.split(data, [int(.8*len(data)), int(.9*len(data))])\n",
        "# create train-valid-test vectors\n",
        "trainX, trainY = vectorize(data_train, sentence_maxlen, w2i)\n",
        "validX, validY = vectorize(data_valid, sentence_maxlen, w2i)\n",
        "testX, testY = vectorize(data_test, sentence_maxlen, w2i)\n",
        "# print info\n",
        "print(f'Loaded {len(data)} data rows with {num_classes} classes.')\n",
        "print(f'Training Data: {len(trainX)}\\nValidation Data: {len(validX)}\\nTest Data: {len(testX)}')\n",
        "print(f'Classes: {classes}')\n",
        "print('Example:')\n",
        "print(data[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Max Length 78\n",
            "Vocab Examples: ['', '!', '!!', '\"', '\"1764\"', '\"74%,', '\">', '\"Activity', '\"BUILD\"', '\"Blue']\n",
            "Vocab Size 12250\n",
            "Loaded 6086 data rows with 3 classes.\n",
            "Training Data: 4868\n",
            "Validation Data: 609\n",
            "Test Data: 609\n",
            "Classes: [0, 1, 2]\n",
            "Example:\n",
            "(['I', 'took', 'it', 'back', 'for', 'an', 'Asus', 'and', 'same', 'thing-', 'blue', 'screen', 'which', 'required', 'me', 'to', 'remove', 'the', 'battery', 'to', 'reset.'], 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkEzFdHOrQQC",
        "colab_type": "text"
      },
      "source": [
        "## **Load Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGTSAJ0_JB28",
        "colab_type": "code",
        "outputId": "ee8e7773-48e9-468d-85fa-7a96696cb7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def load_glove_weights(glove_dir, embd_dim, vocab_size, word_index):\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(glove_dir, 'glove.6B.' + str(embd_dim) + 'd.txt'))\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index)) \n",
        "    embedding_matrix = np.zeros((vocab_size, embd_dim))\n",
        "    print('embed_matrix.shape', embedding_matrix.shape)\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embd_dim = 300\n",
        "glove_embd_w = load_glove_weights('datasets', embd_dim, vocab_size, w2i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "embed_matrix.shape (12250, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NwZdh_4rV2D",
        "colab_type": "text"
      },
      "source": [
        "## **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtdhATLULbMA",
        "colab_type": "code",
        "outputId": "2894c949-782a-41c7-9b68-94972dca2509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "def Net(vocab_size, embd_size, sentence_maxlen, glove_embd_w, num_classes):\n",
        "    sentence = Input((sentence_maxlen,), name='SentenceInput')\n",
        "    \n",
        "    # embedding\n",
        "    embd_layer = Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embd_size, \n",
        "                           weights=[glove_embd_w], \n",
        "                           trainable=False,\n",
        "                           name='shared_embd')\n",
        "    embd_sentence = embd_layer(sentence)\n",
        "    embd_sentence = Permute((2,1))(embd_sentence)\n",
        "    embd_sentence = Lambda(lambda x: K.expand_dims(x, -1))(embd_sentence)\n",
        "    \n",
        "    # cnn\n",
        "    cnn = Conv2D(1, \n",
        "                 kernel_size=(3, sentence_maxlen),\n",
        "                 activation='relu')(embd_sentence)\n",
        "    cnn =  Lambda(lambda x: K.sum(x, axis=3))(cnn)\n",
        "    cnn = MaxPooling1D(3)(cnn)\n",
        "    cnn = Lambda(lambda x: K.sum(x, axis=2))(cnn)\n",
        "    dense = Dense(64, activation='relu')(cnn)\n",
        "    dropout = Dropout(0.2)(dense)\n",
        "    dense = Dense(32, activation='relu')(dropout)\n",
        "    out = Dense(num_classes, activation='softmax')(dense)\n",
        "    adam = Adam(lr=0.0001,\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=None,\n",
        "                decay=0.0,\n",
        "                amsgrad=True)\n",
        "    model = Model(inputs=sentence, outputs=out, name='sentence_claccification')\n",
        "    model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "model = Net(vocab_size, embd_dim, sentence_maxlen, glove_embd_w, num_classes)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sentence_claccification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "SentenceInput (InputLayer)   (None, 78)                0         \n",
            "_________________________________________________________________\n",
            "shared_embd (Embedding)      (None, 78, 300)           3675000   \n",
            "_________________________________________________________________\n",
            "permute_5 (Permute)          (None, 300, 78)           0         \n",
            "_________________________________________________________________\n",
            "lambda_13 (Lambda)           (None, 300, 78, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 298, 1, 1)         235       \n",
            "_________________________________________________________________\n",
            "lambda_14 (Lambda)           (None, 298, 1)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 99, 1)             0         \n",
            "_________________________________________________________________\n",
            "lambda_15 (Lambda)           (None, 99)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 64)                6400      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 3,683,814\n",
            "Trainable params: 8,814\n",
            "Non-trainable params: 3,675,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFxkeBfJrkVa",
        "colab_type": "text"
      },
      "source": [
        "## **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec1QOlrfMTJl",
        "colab_type": "code",
        "outputId": "c3a912bb-f957-4f95-e792-f73fa51f7fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(trainX, trainY,\n",
        "          batch_size=256,\n",
        "          epochs=50,\n",
        "          validation_data=(validX, validY))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4868 samples, validate on 609 samples\n",
            "Epoch 1/50\n",
            "4868/4868 [==============================] - 4s 783us/step - loss: 1.0146 - acc: 0.5306 - val_loss: 0.9681 - val_acc: 0.5353\n",
            "Epoch 2/50\n",
            "4868/4868 [==============================] - 3s 667us/step - loss: 0.9835 - acc: 0.5419 - val_loss: 0.9872 - val_acc: 0.5353\n",
            "Epoch 3/50\n",
            "4868/4868 [==============================] - 3s 665us/step - loss: 0.9631 - acc: 0.5551 - val_loss: 0.9575 - val_acc: 0.5649\n",
            "Epoch 4/50\n",
            "4868/4868 [==============================] - 3s 667us/step - loss: 0.9375 - acc: 0.5826 - val_loss: 0.9802 - val_acc: 0.5287\n",
            "Epoch 5/50\n",
            "4868/4868 [==============================] - 3s 666us/step - loss: 0.9265 - acc: 0.5848 - val_loss: 0.9731 - val_acc: 0.5616\n",
            "Epoch 6/50\n",
            "4868/4868 [==============================] - 3s 667us/step - loss: 0.9065 - acc: 0.5970 - val_loss: 0.9756 - val_acc: 0.5599\n",
            "Epoch 7/50\n",
            "4868/4868 [==============================] - 3s 668us/step - loss: 0.9016 - acc: 0.6035 - val_loss: 1.0063 - val_acc: 0.5517\n",
            "Epoch 8/50\n",
            "4868/4868 [==============================] - 3s 661us/step - loss: 0.8912 - acc: 0.6099 - val_loss: 0.9164 - val_acc: 0.6125\n",
            "Epoch 9/50\n",
            "4868/4868 [==============================] - 3s 659us/step - loss: 0.8788 - acc: 0.6122 - val_loss: 0.9130 - val_acc: 0.5747\n",
            "Epoch 10/50\n",
            "4868/4868 [==============================] - 3s 657us/step - loss: 0.8707 - acc: 0.6189 - val_loss: 0.8922 - val_acc: 0.5911\n",
            "Epoch 11/50\n",
            "4868/4868 [==============================] - 3s 664us/step - loss: 0.8654 - acc: 0.6196 - val_loss: 0.8993 - val_acc: 0.6043\n",
            "Epoch 12/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8604 - acc: 0.6228 - val_loss: 0.9221 - val_acc: 0.5583\n",
            "Epoch 13/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8606 - acc: 0.6237 - val_loss: 0.8645 - val_acc: 0.6388\n",
            "Epoch 14/50\n",
            "4868/4868 [==============================] - 3s 665us/step - loss: 0.8541 - acc: 0.6290 - val_loss: 0.9129 - val_acc: 0.5764\n",
            "Epoch 15/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8564 - acc: 0.6253 - val_loss: 0.9228 - val_acc: 0.5764\n",
            "Epoch 16/50\n",
            "4868/4868 [==============================] - 3s 655us/step - loss: 0.8487 - acc: 0.6311 - val_loss: 0.9443 - val_acc: 0.5550\n",
            "Epoch 17/50\n",
            "4868/4868 [==============================] - 3s 662us/step - loss: 0.8556 - acc: 0.6212 - val_loss: 0.9027 - val_acc: 0.5747\n",
            "Epoch 18/50\n",
            "4868/4868 [==============================] - 3s 654us/step - loss: 0.8460 - acc: 0.6253 - val_loss: 0.8744 - val_acc: 0.6092\n",
            "Epoch 19/50\n",
            "4868/4868 [==============================] - 3s 657us/step - loss: 0.8438 - acc: 0.6313 - val_loss: 0.8294 - val_acc: 0.6338\n",
            "Epoch 20/50\n",
            "4868/4868 [==============================] - 3s 663us/step - loss: 0.8344 - acc: 0.6333 - val_loss: 0.8307 - val_acc: 0.6355\n",
            "Epoch 21/50\n",
            "4868/4868 [==============================] - 3s 655us/step - loss: 0.8355 - acc: 0.6360 - val_loss: 0.8703 - val_acc: 0.6026\n",
            "Epoch 22/50\n",
            "4868/4868 [==============================] - 3s 656us/step - loss: 0.8334 - acc: 0.6319 - val_loss: 0.8407 - val_acc: 0.6207\n",
            "Epoch 23/50\n",
            "4868/4868 [==============================] - 3s 668us/step - loss: 0.8318 - acc: 0.6352 - val_loss: 0.8249 - val_acc: 0.6322\n",
            "Epoch 24/50\n",
            "4868/4868 [==============================] - 3s 679us/step - loss: 0.8205 - acc: 0.6405 - val_loss: 0.8277 - val_acc: 0.6273\n",
            "Epoch 25/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8279 - acc: 0.6378 - val_loss: 0.8580 - val_acc: 0.6125\n",
            "Epoch 26/50\n",
            "4868/4868 [==============================] - 3s 657us/step - loss: 0.8297 - acc: 0.6409 - val_loss: 0.8262 - val_acc: 0.6305\n",
            "Epoch 27/50\n",
            "4868/4868 [==============================] - 3s 663us/step - loss: 0.8201 - acc: 0.6475 - val_loss: 0.8683 - val_acc: 0.5961\n",
            "Epoch 28/50\n",
            "4868/4868 [==============================] - 3s 657us/step - loss: 0.8277 - acc: 0.6362 - val_loss: 0.9827 - val_acc: 0.5172\n",
            "Epoch 29/50\n",
            "4868/4868 [==============================] - 3s 656us/step - loss: 0.8309 - acc: 0.6366 - val_loss: 0.8239 - val_acc: 0.6273\n",
            "Epoch 30/50\n",
            "4868/4868 [==============================] - 3s 668us/step - loss: 0.8169 - acc: 0.6493 - val_loss: 0.8290 - val_acc: 0.6338\n",
            "Epoch 31/50\n",
            "4868/4868 [==============================] - 3s 662us/step - loss: 0.8152 - acc: 0.6493 - val_loss: 0.8193 - val_acc: 0.6322\n",
            "Epoch 32/50\n",
            "4868/4868 [==============================] - 3s 661us/step - loss: 0.8149 - acc: 0.6454 - val_loss: 0.8451 - val_acc: 0.6076\n",
            "Epoch 33/50\n",
            "4868/4868 [==============================] - 3s 665us/step - loss: 0.8125 - acc: 0.6537 - val_loss: 0.8207 - val_acc: 0.6355\n",
            "Epoch 34/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8145 - acc: 0.6434 - val_loss: 0.8685 - val_acc: 0.5993\n",
            "Epoch 35/50\n",
            "4868/4868 [==============================] - 3s 655us/step - loss: 0.8078 - acc: 0.6487 - val_loss: 0.8609 - val_acc: 0.6026\n",
            "Epoch 36/50\n",
            "4868/4868 [==============================] - 3s 669us/step - loss: 0.8112 - acc: 0.6452 - val_loss: 0.8435 - val_acc: 0.6207\n",
            "Epoch 37/50\n",
            "4868/4868 [==============================] - 3s 659us/step - loss: 0.8029 - acc: 0.6555 - val_loss: 0.8815 - val_acc: 0.5961\n",
            "Epoch 38/50\n",
            "4868/4868 [==============================] - 3s 660us/step - loss: 0.8125 - acc: 0.6413 - val_loss: 0.8449 - val_acc: 0.6158\n",
            "Epoch 39/50\n",
            "4868/4868 [==============================] - 3s 662us/step - loss: 0.8034 - acc: 0.6498 - val_loss: 0.8239 - val_acc: 0.6355\n",
            "Epoch 40/50\n",
            "4868/4868 [==============================] - 3s 661us/step - loss: 0.8024 - acc: 0.6530 - val_loss: 0.8215 - val_acc: 0.6322\n",
            "Epoch 41/50\n",
            "4868/4868 [==============================] - 3s 661us/step - loss: 0.8065 - acc: 0.6535 - val_loss: 0.8135 - val_acc: 0.6470\n",
            "Epoch 42/50\n",
            "4868/4868 [==============================] - 3s 658us/step - loss: 0.7984 - acc: 0.6537 - val_loss: 0.8119 - val_acc: 0.6371\n",
            "Epoch 43/50\n",
            "4868/4868 [==============================] - 3s 666us/step - loss: 0.8024 - acc: 0.6512 - val_loss: 0.8190 - val_acc: 0.6322\n",
            "Epoch 44/50\n",
            "4868/4868 [==============================] - 3s 655us/step - loss: 0.7975 - acc: 0.6547 - val_loss: 0.8350 - val_acc: 0.6305\n",
            "Epoch 45/50\n",
            "4868/4868 [==============================] - 3s 658us/step - loss: 0.8004 - acc: 0.6551 - val_loss: 0.8271 - val_acc: 0.6305\n",
            "Epoch 46/50\n",
            "4868/4868 [==============================] - 3s 659us/step - loss: 0.7991 - acc: 0.6522 - val_loss: 0.8299 - val_acc: 0.6338\n",
            "Epoch 47/50\n",
            "4868/4868 [==============================] - 3s 654us/step - loss: 0.7992 - acc: 0.6565 - val_loss: 0.8559 - val_acc: 0.6043\n",
            "Epoch 48/50\n",
            "4868/4868 [==============================] - 3s 652us/step - loss: 0.7992 - acc: 0.6524 - val_loss: 0.8317 - val_acc: 0.6223\n",
            "Epoch 49/50\n",
            "4868/4868 [==============================] - 3s 659us/step - loss: 0.7947 - acc: 0.6594 - val_loss: 0.8221 - val_acc: 0.6486\n",
            "Epoch 50/50\n",
            "4868/4868 [==============================] - 3s 658us/step - loss: 0.7902 - acc: 0.6555 - val_loss: 0.8424 - val_acc: 0.6305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f888b52d320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcPOHQ1Wd67L",
        "colab_type": "code",
        "outputId": "bdcaa346-15b0-47cb-e9ed-d330a5d742fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "results = model.evaluate(testX, testY)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "609/609 [==============================] - 0s 248us/step\n",
            "Test accuracy:  0.5763546798029556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnnUVsrItbUO",
        "colab_type": "text"
      },
      "source": [
        "# **Results: Accuracy**\n",
        "\n",
        "## **SST-1**: 5 classes\n",
        "#### Training: 39.31%\n",
        "#### Validation: 36.30%\n",
        "#### Testing: 37.22%\n",
        "\n",
        "## **SST-2**: 2 classes\n",
        "#### Training: 74.45%\n",
        "#### Validation: 72.16%\n",
        "#### Testing: 72.39%\n",
        "\n",
        "## **SentiHood**: 2 classes\n",
        "#### Training: 80.65%\n",
        "#### Validation: 72.87%\n",
        "#### Testing: 72.18%\n",
        "\n",
        "## **SemEval**: 3 classes\n",
        "#### Training: 65.55%\n",
        "#### Validation: 63.05%\n",
        "#### Testing: 57.64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7d-UTKtxNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}