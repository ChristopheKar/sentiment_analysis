{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcnn-sentence-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbzGeVgKM-",
        "colab_type": "text"
      },
      "source": [
        "## **Source**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4SRMLXFnqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/mk60991/DeepLearning-with-keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnho0UXKyYZi",
        "colab_type": "text"
      },
      "source": [
        "## **Setting up environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRXdLgb0JBS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip datasets.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmiE8Ynyss3",
        "colab_type": "text"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBi9BdtMdvZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.datasets import imdb\n",
        "from keras.engine import Layer, InputSpec\n",
        "import tensorflow as tf\n",
        "import xml.etree.ElementTree as ET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXs0RNLd_ZOB",
        "colab_type": "text"
      },
      "source": [
        "## **Loading Data from Different Datasets**\n",
        "To train for a specific dataset, only run the cell for that dataset, then skip over to the next section.\\\n",
        "Don't forget to run the first cells in this section for the loading functions.\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLZ0u96sF3rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_vocab(sentences):\n",
        "    vectorizer = CountVectorizer(input = u'content',\n",
        "                                analyzer = \"word\",\n",
        "                                tokenizer = None,\n",
        "                                preprocessor = None,\n",
        "                                stop_words = None,\n",
        "                                token_pattern = r',|\\b\\w+\\b',\n",
        "                                max_features = 15448)\n",
        "    vectorizer.fit(sentences)\n",
        "    return vectorizer.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abcwdh0gKpDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sst-1 data\n",
        "def load_vocab(paths):\n",
        "    sentences = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "            r = f.read()\n",
        "        for line in r.split('\\n'):\n",
        "            line = line.rstrip()\n",
        "            text = line.split(' ')[1:]\n",
        "            sentences.append(' '.join(text))\n",
        "    vocab = generate_vocab(sentences)\n",
        "    return vocab\n",
        "\n",
        "def load_data(split_path, vocab):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(split_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "        r = f.read()\n",
        "    for line in r.split('\\n'):\n",
        "        line = line.rstrip()\n",
        "        text = line.split(' ')[1:]\n",
        "        label = int(line.split(' ')[0])\n",
        "        mapped_sentence = []\n",
        "        for token in text:\n",
        "            if token in vocab:\n",
        "                mapped_sentence.append(str(vocab[token]))\n",
        "            else:\n",
        "                mapped_sentence.append(15448)\n",
        "        labels.append(label)\n",
        "        data.append(mapped_sentence)\n",
        "    return np.array(data), to_categorical(np.array(labels)), len(set(labels))\n",
        "\n",
        "vocab = load_vocab(['datasets/sst/Training_SST-1.txt', 'datasets/sst/Dev_SST-1.txt', 'datasets/sst/Test_SST-1.txt'])\n",
        "X_train, y_train, num_classes = load_data('datasets/sst/Training_SST-1.txt', vocab)\n",
        "X_valid, y_valid, _ = load_data('datasets/sst/Dev_SST-1.txt', vocab)\n",
        "X_test, y_test, _ = load_data('datasets/sst/Test_SST-1.txt', vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ert6poEnK1X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sst-2 data\n",
        "def load_vocab(paths):\n",
        "    sentences = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "            r = f.read()\n",
        "        for line in r.split('\\n'):\n",
        "            line = line.rstrip()\n",
        "            text = line.split(' ')[1:]\n",
        "            sentences.append(' '.join(text))\n",
        "    vocab = generate_vocab(sentences)\n",
        "    return vocab\n",
        "\n",
        "def load_data(split_path, vocab):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(split_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
        "        r = f.read()\n",
        "    for line in r.split('\\n'):\n",
        "        line = line.rstrip()\n",
        "        text = line.split(' ')[1:]\n",
        "        label = int(line.split(' ')[0])\n",
        "        mapped_sentence = []\n",
        "        for token in text:\n",
        "            if token in vocab:\n",
        "                mapped_sentence.append(str(vocab[token]))\n",
        "            else:\n",
        "                mapped_sentence.append(15448)\n",
        "        labels.append(label)\n",
        "        data.append(mapped_sentence)\n",
        "    return np.array(data), to_categorical(np.array(labels)), len(set(labels))\n",
        "\n",
        "vocab = load_vocab(['datasets/sst/Training_SST-2.txt', 'datasets/sst/Dev_SST-2.txt', 'datasets/sst/Test_SST-2.txt'])\n",
        "X_train, y_train, num_classes = load_data('datasets/sst/Training_SST-2.txt', vocab)\n",
        "X_valid, y_valid, _ = load_data('datasets/sst/Dev_SST-2.txt', vocab)\n",
        "X_test, y_test, _ = load_data('datasets/sst/Test_SST-2.txt', vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAb7KgqjXtRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentihood data\n",
        "def load_vocab(paths):\n",
        "    sentences = []\n",
        "    for path in paths:\n",
        "        with open(path, 'r') as f:\n",
        "            j = json.load(f)\n",
        "        for i in range(len(j)):\n",
        "            if len(j[i]['opinions']) > 0:\n",
        "                text = j[i]['text'].strip()\n",
        "                for idx in range(len(j[i]['opinions'])):\n",
        "                    aspect = j[i]['opinions'][idx]['aspect']\n",
        "                    entity = j[i]['opinions'][idx]['target_entity']\n",
        "                    text = text.replace(entity, aspect)\n",
        "                sentences.append(' '.join(text))\n",
        "    vocab = generate_vocab(sentences)\n",
        "    return vocab\n",
        "\n",
        "def load_data(split_path, vocab):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(split_path, 'r') as f:\n",
        "        j = json.load(f)\n",
        "        for i in range(len(j)):\n",
        "            if len(j[i]['opinions']) > 0:\n",
        "                if j[i]['opinions'][0]['sentiment'] == 'Positive':\n",
        "                    label = 1\n",
        "                else:\n",
        "                    label = 0\n",
        "                text = j[i]['text'].strip()\n",
        "                for idx in range(len(j[i]['opinions'])):\n",
        "                    aspect = j[i]['opinions'][idx]['aspect']\n",
        "                    entity = j[i]['opinions'][idx]['target_entity']\n",
        "                    text = text.replace(entity, aspect)\n",
        "                mapped_sentence = []\n",
        "                for token in text:\n",
        "                    if token in vocab:\n",
        "                        mapped_sentence.append(str(vocab[token]))\n",
        "                    else:\n",
        "                        mapped_sentence.append(15448)\n",
        "                labels.append(label)\n",
        "                data.append(mapped_sentence)\n",
        "    return np.array(data), to_categorical(np.array(labels)), len(set(labels))\n",
        "\n",
        "vocab = load_vocab(['datasets/sentihood/sentihood-train.json', 'datasets/sentihood/sentihood-dev.json', 'datasets/sentihood/sentihood-test.json'])\n",
        "X_train, y_train, num_classes = load_data('datasets/sentihood/sentihood-train.json', vocab)\n",
        "X_valid, y_valid, _ = load_data('datasets/sentihood/sentihood-dev.json', vocab)\n",
        "X_test, y_test, _ = load_data('datasets/sentihood/sentihood-test.json', vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQeQDy_AZFj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# semeval data\n",
        "def load_vocab(paths):\n",
        "    sentences = []\n",
        "    for path in paths:\n",
        "        tree = ET.parse(path)\n",
        "        root = tree.getroot()\n",
        "        for sentence in root.iter('sentence'):\n",
        "            text = sentence.find('text').text.strip()\n",
        "            sentences.append(text)\n",
        "    vocab = generate_vocab(sentences)\n",
        "    return vocab\n",
        "\n",
        "def load_data(paths, vocab):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for split_path in paths:\n",
        "        tree = ET.parse(split_path)\n",
        "        root = tree.getroot()\n",
        "        for sentence in root.iter('sentence'):\n",
        "            text = sentence.find('text').text.strip()\n",
        "            P = []\n",
        "            aspects = sentence.find('aspectTerms')\n",
        "            if aspects is not None:\n",
        "                for aspect in aspects:\n",
        "                    P.append(aspect.get('polarity'))\n",
        "            if 'negative' in P and not 'positive' in P:\n",
        "                polarity = 0\n",
        "            elif 'positive' in P and not 'negative' in P:\n",
        "                polarity = 1\n",
        "            else:\n",
        "                polarity = 2\n",
        "            mapped_sentence = []\n",
        "            for token in text:\n",
        "                if token in vocab:\n",
        "                    mapped_sentence.append(str(vocab[token]))\n",
        "                else:\n",
        "                    mapped_sentence.append(15448)\n",
        "            labels.append(polarity)\n",
        "            data.append(mapped_sentence)\n",
        "    return np.array(data), to_categorical(np.array(labels)), len(set(labels))\n",
        "\n",
        "vocab = load_vocab(['datasets/semeval/Laptops_train_v2.xml.txt', 'datasets/semeval/Restaurants_Train_v2.xml.txt'])\n",
        "data, labels, num_classes = load_data(['datasets/semeval/Laptops_train_v2.xml.txt', 'datasets/semeval/Restaurants_Train_v2.xml.txt'], vocab)\n",
        "X_train, X_valid, X_test = np.split(data, [int(.8*len(data)), int(.9*len(data))])\n",
        "y_train, y_valid, y_test = np.split(labels, [int(.8*len(labels)), int(.9*len(labels))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPZLrY1D_ljk",
        "colab_type": "text"
      },
      "source": [
        "## **Training options and data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwiehB33KpCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 20000\n",
        "max_len = 300\n",
        "embedding_dim = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6hjxzvYKlzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "X_valid = sequence.pad_sequences(X_valid, maxlen=max_len)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "print(X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItxQkjbJ_q2I",
        "colab_type": "text"
      },
      "source": [
        "## **Basic Dynamic CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMHBakoNKhD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KMaxPooling(Layer):\n",
        "    \"\"\"\n",
        "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
        "    TensorFlow backend.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(ndim=3)\n",
        "        self.k = k\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], (input_shape[1] * self.k))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # swap last two dimensions since top_k will be applied along the last dimension\n",
        "        #shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
        "        \n",
        "        # extract top_k, returns two tensors [values, indices]\n",
        "        top_k = tf.nn.top_k(inputs, k=self.k, sorted=True, name=None)[0]\n",
        "        \n",
        "        # return flattened output\n",
        "        return top_k\n",
        "  \n",
        "\n",
        "def basic_dynamic_cnn(k = 5):\n",
        "    model = Sequential()\n",
        "    # Embedding each word\n",
        "    model.add(Embedding(num_words, embedding_dim, input_length = max_len))\n",
        "    # Wide convolution\n",
        "    model.add(ZeroPadding1D(29))\n",
        "    model.add(Conv1D(embedding_dim, 30, activation = 'relu'))\n",
        "    # k-max pooling\n",
        "    model.add(Permute((2, 1)))\n",
        "    model.add(KMaxPooling(k))\n",
        "    model.add(Reshape((k, -1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation = 'softmax'))\n",
        "    \n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFGZUbf3JS5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "basic_dynamic_cnn = basic_dynamic_cnn()\n",
        "callbacks = [ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_acc', verbose=1, save_best_only = True, mode='max')]\n",
        "history = basic_dynamic_cnn.fit(X_train, y_train, callbacks = callbacks, epochs = 10, validation_data = (X_valid, y_valid), batch_size = 200)\n",
        "results = basic_dynamic_cnn.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])\n",
        "print('=============================')\n",
        "print('Training: ', round(history.history['acc'][-1]*100, 2), '%')\n",
        "print('Validation: ', round(history.history['val_acc'][-1]*100, 2), '%')\n",
        "print('Testing: ', round(results[1]*100, 2), '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8kB4Mi5DPtq",
        "colab_type": "text"
      },
      "source": [
        "## **Two Conv Dynamic CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_UDk7kaJtLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# two kinds of k's and kernel sizes for each operation\n",
        "def two_conv_dynamic_cnn(k1 = 20, k2 = 10, ksize1 = 20, ksize2 = 30):\n",
        "    inputs = Input(shape = (X_train.shape[-1],))\n",
        "    embed = Embedding(num_words, embedding_dim, input_length = max_len)(inputs)\n",
        "    padded = ZeroPadding1D(ksize1 - 1)(embed)\n",
        "    conv1 = Conv1D(embedding_dim, ksize1, activation = 'relu')(padded)\n",
        "    permuted = Permute((2,1))(conv1)\n",
        "    kmaxpool1 = KMaxPooling(k1)(permuted)\n",
        "    kmaxpool1 = Reshape((k1, -1))(kmaxpool1)\n",
        "    padded = ZeroPadding1D(ksize2 -1)(kmaxpool1)\n",
        "    conv2 = Conv1D(embedding_dim, ksize2, activation = 'relu')(padded)\n",
        "    permuted = Permute((2,1))(conv2)\n",
        "    kmaxpool2 = KMaxPooling(k2)(permuted)\n",
        "    kmaxpool2 = Reshape((k2, -1))(kmaxpool2)\n",
        "    flattened = Flatten()(kmaxpool2)\n",
        "    outputs = Dense(num_classes, activation = 'softmax')(flattened)\n",
        "    \n",
        "    model = Model(inputs = inputs, outputs = outputs)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6J3r8PDLOj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "two_conv_dynamic_cnn = two_conv_dynamic_cnn()\n",
        "callbacks = [ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_acc', verbose=1, save_best_only = True, mode='max')]\n",
        "history = two_conv_dynamic_cnn.fit(X_train, y_train, callbacks = callbacks, epochs = 10, validation_data = (X_valid, y_valid), batch_size = 200)\n",
        "results = two_conv_dynamic_cnn.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])\n",
        "print('=============================')\n",
        "print('Training: ', round(history.history['acc'][-1]*100, 2), '%')\n",
        "print('Validation: ', round(history.history['val_acc'][-1]*100, 2), '%')\n",
        "print('Testing: ', round(results[1]*100, 2), '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8BFfPi1D3o3",
        "colab_type": "text"
      },
      "source": [
        "## **Two Feature Map Dynamic CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMU6nNUELbKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def two_feature_map_dynamic_cnn(k1 = 20, k2 = 10, ksize1 = 20, ksize2 = 30):\n",
        "    inputs = Input(shape = (X_train.shape[-1],))\n",
        "    embed = Embedding(num_words, embedding_dim, input_length = max_len)(inputs)\n",
        "    conv_results = []\n",
        "    # two feature maps using for loop\n",
        "    for i in range(2):\n",
        "        padded = ZeroPadding1D(ksize1 - 1)(embed)\n",
        "        conv1 = Conv1D(embedding_dim, ksize1, activation = 'relu')(padded)\n",
        "        permuted = Permute((2,1))(conv1)\n",
        "        kmaxpool1 = KMaxPooling(k1)(permuted)\n",
        "        kmaxpool1 = Reshape((k1, -1))(kmaxpool1)\n",
        "        padded = ZeroPadding1D(ksize2 -1)(kmaxpool1)\n",
        "        conv2 = Conv1D(embedding_dim, ksize2, activation = 'relu')(padded)\n",
        "        permuted = Permute((2,1))(conv2)\n",
        "        kmaxpool2 = KMaxPooling(k2)(permuted)\n",
        "        kmaxpool2 = Reshape((k2, -1))(kmaxpool2)\n",
        "        flattened = Flatten()(kmaxpool2)\n",
        "        conv_results.append(flattened)\n",
        "    conv_result = concatenate(conv_results)\n",
        "    outputs = Dense(num_classes, activation = 'softmax')(conv_result)\n",
        "    \n",
        "    model = Model(inputs = inputs, outputs = outputs)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJgLXwEnLhv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "two_feature_map_dynamic_cnn = two_feature_map_dynamic_cnn()\n",
        "callbacks = [ModelCheckpoint(filepath = 'best_model.hdf5', monitor='val_acc', verbose=1, save_best_only = True, mode='max')]\n",
        "history = two_feature_map_dynamic_cnn.fit(X_train, y_train, callbacks = callbacks, epochs = 10, validation_data = (X_valid, y_valid), batch_size = 200)\n",
        "results = two_feature_map_dynamic_cnn.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])\n",
        "print('=============================')\n",
        "print('Training:', round(history.history['acc'][-1]*100, 2), '%')\n",
        "print('Validation:', round(history.history['val_acc'][-1]*100, 2), '%')\n",
        "print('Testing:', round(results[1]*100, 2), '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc99rxSQFTdI",
        "colab_type": "text"
      },
      "source": [
        "# **Results: Accuracy**\n",
        "\n",
        "\n",
        "## **SST-1**: 5 classes\n",
        "\n",
        "### Basic Dynamic CNN:\n",
        "#### Training: 71.10%\n",
        "#### Validation: 57.64%\n",
        "#### Testing: 58.29%\n",
        "\n",
        "## Two-Conv Dynamic CNN:\n",
        "#### Training: 63.66%\n",
        "#### Validation: 59.93%\n",
        "#### Testing: 57.64%\n",
        "\n",
        "## Two-Feature-Map Dynamic CNN:\n",
        "#### Training: 65.18%\n",
        "#### Validation: 53.69%\n",
        "#### Testing: 56.48%\n",
        "\n",
        "\n",
        "## **SST-2**: 2 classes\n",
        "\n",
        "### Basic Dynamic CNN:\n",
        "#### Training: 99.86%\n",
        "#### Validation: 76.40%\n",
        "#### Testing: 78.32%\n",
        "\n",
        "## Two-Conv Dynamic CNN:\n",
        "#### Training: 99.71%\n",
        "#### Validation: 76.06%\n",
        "#### Testing: 77.06%\n",
        "\n",
        "## Two-Feature-Map Dynamic CNN:\n",
        "#### Training: 99.81%\n",
        "#### Validation: 78.01%\n",
        "#### Testing: 77.66%\n",
        "\n",
        "## **SentiHood**: 2 classes\n",
        "\n",
        "### Basic Dynamic CNN:\n",
        "#### Training: 76.94 %\n",
        "#### Validation: 75.84 %\n",
        "#### Testing: 73.78 %\n",
        "\n",
        "## Two-Conv Dynamic CNN:\n",
        "#### Training: 75.9 %\n",
        "#### Validation: 75.25 %\n",
        "#### Testing: 72.98 %\n",
        "\n",
        "## Two-Feature-Map Dynamic CNN:\n",
        "#### Training: 74.37 %\n",
        "#### Validation: 74.65 %\n",
        "#### Testing: 73.08 %\n",
        "\n",
        "## **SemEval**: 3 classes\n",
        "\n",
        "### Basic Dynamic CNN:\n",
        "#### Training: 73.77 %\n",
        "#### Validation: 60.43 %\n",
        "#### Testing: 61.08 %\n",
        "\n",
        "## Two-Conv Dynamic CNN:\n",
        "#### Training: 64.11 %\n",
        "#### Validation: 58.62 %\n",
        "#### Testing: 57.96 %\n",
        "\n",
        "## Two-Feature-Map Dynamic CNN:\n",
        "#### Training: 62.82 %\n",
        "#### Validation: 55.67 %\n",
        "#### Testing: 56.81 %"
      ]
    }
  ]
}